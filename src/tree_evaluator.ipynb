{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tree_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-lender",
   "metadata": {},
   "source": [
    "# tree_evaluator\n",
    "> Tree evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import functools\n",
    "from pyuid3.att_stats import AttStats\n",
    "from pyuid3.attribute import Attribute\n",
    "from pyuid3.data import Data\n",
    "from pyuid3.tree import Tree\n",
    "from pyuid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TreeEvaluator:\n",
    "    class Stats:\n",
    "        def __init__(self, class_label):\n",
    "            self.class_label = class_label\n",
    "            self.TP = 0\n",
    "            self.FP = 0\n",
    "            self.TN = 0\n",
    "            self.FN = 0\n",
    "\n",
    "        def get_TP_rate(self) -> float:\n",
    "            return self.get_TP() / (self.get_TP() + self.get_FN())\n",
    "\n",
    "        def get_FP_rate(self) -> float:\n",
    "            return self.get_FP() / (self.get_FP() + self.get_TN())\n",
    "\n",
    "        def get_precision(self) -> float:\n",
    "            return self.get_TP() / (self.get_TP() + self.get_FP())\n",
    "\n",
    "        def get_recall(self) -> float:\n",
    "            return self.get_TP() / (self.get_TP() + self.get_FN())\n",
    "\n",
    "        def get_F_measure(self) -> float:\n",
    "            return 2 * (self.get_precision() * self.get_recall()) / (self.get_precision() + self.get_recall())\n",
    "\n",
    "\n",
    "        def get_ROC_area(self, bench) -> float:\n",
    "            result = 0\n",
    "            preds = bench.get_predictions()\n",
    "            \n",
    "            \n",
    "            def compare(p1, p2):\n",
    "                prob1 = p1.prediction.get_stat_for_value(self.class_label)\n",
    "                prob2 = p2.prediction.get_stat_for_value(self.class_label)\n",
    "                if prob1 > prob2:\n",
    "                    return 1\n",
    "                elif prob1 < prob2:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            #---- Calling\n",
    "            preds = sorted(preds, key=functools.cmp_to_key(compare))\n",
    "            # ------\n",
    "\n",
    "            n_class_count = 0\n",
    "            y_class_count = 0\n",
    "            uy  = 0\n",
    "            un = 0\n",
    "            for p in preds:\n",
    "                if p.correct_label == self.class_label:\n",
    "                    uy += n_class_count\n",
    "                    y_class_count += 1\n",
    "                else:\n",
    "                    un += y_class_count\n",
    "                    n_class_count += 1\n",
    "\n",
    "            result = uy / (uy + un)\n",
    "            return result\n",
    "\n",
    "        def get_class_label(self) -> str:\n",
    "            return self.class_label\n",
    "\n",
    "        def set_class_label(class_label: str) -> None:\n",
    "            self.class_label = class_label\n",
    "\n",
    "        def get_TP(self) -> float:\n",
    "            return self.TP\n",
    "\n",
    "        def set_TP(self, TP: float) -> None:\n",
    "            self.TP = TP\n",
    "\n",
    "        def get_FP(self) -> float:\n",
    "            return self.FP\n",
    "\n",
    "        def set_FP(self, FP: float) -> None:\n",
    "            self.FP = FP\n",
    "\n",
    "        def get_TN(self) -> float:\n",
    "            return self.TN\n",
    "\n",
    "        def set_TN(self, TN: float) -> None:\n",
    "            self.TN = TN\n",
    "\n",
    "        def get_FN(self) -> float:\n",
    "            return self.FN\n",
    "\n",
    "        def set_FN(self, FN: float) -> None:\n",
    "            self.FN = FN\n",
    "\n",
    "    class BenchmarkResult:\n",
    "        def __init__(self, class_attribute: Attribute):\n",
    "            self.stats = []\n",
    "            self.predictions = []\n",
    "\n",
    "            self.stats = [TreeEvaluator.Stats(class_label) for class_label in class_attribute.get_domain()]\n",
    "\n",
    "            self.correct = 0\n",
    "            self.incorrect = 0\n",
    "\n",
    "        def get_predictions(self) -> list:\n",
    "            return self.predictions\n",
    "\n",
    "        def get_accuracy(self) -> float:\n",
    "            return self.correct / (self.correct + self.incorrect)\n",
    "\n",
    "        def get_stats_for_label(self, class_label: str): \n",
    "            for s in self.stats:\n",
    "                if s.get_class_label() == class_label:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        def add_TP(self, value: str) -> None:\n",
    "            for s in self.stats:\n",
    "                if s.get_class_label() == value: \n",
    "                    s.set_TP(s.get_TP() + 1)\n",
    "                    break\n",
    "\n",
    "        def add_FP(self, value: str) -> None:\n",
    "            for s in self.stats:\n",
    "                if s.get_class_label() == value: \n",
    "                    s.set_FP(s.get_FP() + 1)\n",
    "                    break\n",
    "\n",
    "        def add_TN(self, value: str) -> None:\n",
    "            for s in self.stats:\n",
    "                if s.get_class_label() == value: \n",
    "                    s.set_TN(s.get_TN() + 1)\n",
    "                    break\n",
    "\n",
    "        def add_FN(self, value: str) -> None:\n",
    "            for s in self.stats:\n",
    "                if s.get_class_label() == value: \n",
    "                    s.set_FN(s.get_FN() + 1)\n",
    "                    break\n",
    "\n",
    "        def add_prediction(self, prediction) -> None:\n",
    "            self.predictions.append(prediction)\n",
    "\n",
    "    \n",
    "    class Prediction:\n",
    "        def __init__(self, prediction: AttStats, correct_label: str):\n",
    "            self.prediction = prediction\n",
    "            self.correct_label = correct_label\n",
    "\n",
    "        def __lt__(self, p1):\n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 < prob2\n",
    "\n",
    "        def __le__(self, p1):\n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 <= prob2\n",
    "\n",
    "        def __eq__(self, p1): \n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 == prob2\n",
    "\n",
    "        def __ne__(self, p1): \n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 != prob2\n",
    "\n",
    "        def __gt__(self, p1): \n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 > prob2\n",
    "\n",
    "        def __ge__(self, p1): \n",
    "            prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "            prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "            return prob1 >= prob2\n",
    " \n",
    "\n",
    "    @staticmethod\n",
    "    def train_and_test(training_data: Data, test_data: Data):\n",
    "        #trained_tree = UId3.fit(training_data, UncertainEntropyEvaluator(), 0)\n",
    "\n",
    "        return TreeEvaluator.test(training_data, test_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(trained_tree: 'Tree', test_data: Data):\n",
    "        result = TreeEvaluator.BenchmarkResult(test_data.get_class_attribute())\n",
    "\n",
    "        for i in test_data.get_instances():\n",
    "            prediction = trained_tree.predict(i)\n",
    "            error =  not prediction.get_most_probable().get_name() == i.get_readings()[-1].get_most_probable().get_name()\n",
    "            result.add_prediction(TreeEvaluator.Prediction(prediction, i.get_readings()[-1].get_most_probable().get_name()))\n",
    "            if error:\n",
    "                #give false positive to predicted class, false negative to real class, and true negatives to other\n",
    "                predicted_name = prediction.get_most_probable().get_name()\n",
    "                real_name = i.get_readings()[-1].get_most_probable().get_name()\n",
    "                result.add_FP(predicted_name)\n",
    "                result.add_FN(real_name)\n",
    "                for s in result.stats: \n",
    "                    if not s.class_label == predicted_name and not s.class_label == real_name:\n",
    "                        result.add_TN(s.get_class_label())\n",
    "                result.incorrect += 1 \n",
    "            else:\n",
    "                #add true positive for predicted class, and true negatives for other\n",
    "                predicted_name = prediction.get_most_probable().get_name()\n",
    "                result.add_TP(predicted_name)\n",
    "                for s in result.stats:\n",
    "                    if not s.class_label == predicted_name:\n",
    "                        result.add_TN(s.get_class_label())\n",
    "                result.correct += 1\n",
    "        return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-theme",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-bedroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testabc:\n",
    "    def __init__(self, prediction: int):\n",
    "        self.prediction = prediction\n",
    "        \n",
    "    def __lt__(self, p1):\n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 < prob2\n",
    "    \n",
    "    def __le__(self, p1):\n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 <= prob2\n",
    "    \n",
    "    def __eq__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 == prob2\n",
    "    \n",
    "    def __ne__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 != prob2\n",
    "    \n",
    "    def __gt__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 > prob2\n",
    "    \n",
    "    def __ge__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 >= prob2\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = Testabc(3)\n",
    "test2 = Testabc(63)\n",
    "test3 = Testabc(33)\n",
    "test4 = Testabc(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Testabc at 0x7fe3f0a2bd30>,\n",
       " <__main__.Testabc at 0x7fe3f0a2bc40>,\n",
       " <__main__.Testabc at 0x7fe3f0a2bfa0>,\n",
       " <__main__.Testabc at 0x7fe411c7ff70>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([test1, test2, test3, test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "33\n",
      "63\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "for a in sorted([test1, test2, test3, test4]):\n",
    "    print(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 <test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
