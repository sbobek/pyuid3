{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tree_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-concentration",
   "metadata": {},
   "source": [
    "# tree_evaluator\n",
    "> Tree evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uid3.att_stats import AttStats\n",
    "from uid3.attribute import Attribute\n",
    "from uid3.data import Data\n",
    "from uid3.tree import Tree\n",
    "from uid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TreeEvaluator:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BenchmarkResult:\n",
    "    def __init__(self, class_attribute: Attribute):\n",
    "        self.stats = []\n",
    "        self.predicitons = []\n",
    "        \n",
    "        self.stats = [class_label for class_label in class_attribute.get_domain()]\n",
    "    \n",
    "    def train_and_test(self, training_data: Data, test_data: Data):\n",
    "        trained_tree = UId3.grow_tree(trainingData, UncertainEntropyEvaluator(), 0)\n",
    "        \n",
    "        return self.test(trained_tree, test_data)\n",
    "\n",
    "\n",
    "    def test(trained_tree: Tree, test_data: Data):\n",
    "        result = BenchmarkResult(test_data.get_class_attribute())\n",
    "\n",
    "        for i in test_data.get_instances():\n",
    "            prediction = trained_tree.predict(i)\n",
    "            error =  not prediction.get_most_porbable().get_name() == i.get_readings().get_last().get_most_probable().get_name()\n",
    "            result.add_prediction(Prediction(prediction, i.get_readings().get_last().get_most_probable().get_name()))\n",
    "            if error:\n",
    "                #give false positive to predicted class, false negative to real class, and true negatives to other\n",
    "                predicted_name = prediction.get_most_porbable().get_name()\n",
    "                real_name = i.get_readings().get_last().get_most_probable().get_name()\n",
    "                result.add_FP(predicted_name)\n",
    "                result.add_FN(real_name)\n",
    "                for s in result.stats: \n",
    "                    if not s.class_label == predicted_name and not s.class_label == real_name:\n",
    "                        result.add_TN(s.get_class_label())\n",
    "                result.incorrect += 1 \n",
    "            else:\n",
    "                #add true positive for predicted class, and true negatives for other\n",
    "                predicted_name = prediction.get_most_porbable().get_name()\n",
    "                result.add_TP(predicted_name)\n",
    "                for s in result.stats:\n",
    "                    if not s.class_label == predicted_name:\n",
    "                        result.add_TN(s.get_class_label())\n",
    "                result.correct += 1\n",
    "        return result\n",
    "\n",
    "    def get_predictions(self) -> list:\n",
    "        return predictions\n",
    "    \n",
    "    def get_accuracy(self) -> float:\n",
    "        return correct / (correct + incorrect)\n",
    "    \n",
    "    def get_stats_for_label(self, class_label: str) -> Stats: \n",
    "        for s in self.stats:\n",
    "            if s.get_class_label() == class_label:\n",
    "                return s\n",
    "        return None\n",
    "    \n",
    "    def add_TP(self, value: str) -> None:\n",
    "        for s in self.stats:\n",
    "            if s.get_class_label() == value: \n",
    "                s.set_TP(s.get_TP() + 1)\n",
    "                break\n",
    "\n",
    "    def add_FP(self, value: str) -> None:\n",
    "        for s in self.stats:\n",
    "            if s.get_class_label() == value: \n",
    "                s.set_FP(s.get_FP() + 1)\n",
    "                break\n",
    "\n",
    "    def add_TN(self, value: str) -> None:\n",
    "        for s in self.stats:\n",
    "            if s.get_class_label() == value: \n",
    "                s.set_TN(s.get_TN() + 1)\n",
    "                break\n",
    "   \n",
    "    def add_FN(self, value: str) -> None:\n",
    "        for s in self.stats:\n",
    "            if s.get_class_label() == value: \n",
    "                s.set_FN(s.get_FN() + 1)\n",
    "                break\n",
    "\n",
    "    def add_prediction(self, prediction: Prediction) -> None:\n",
    "        predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Prediction:\n",
    "    def __init__(self, prediction: AttStats, correct_label: str):\n",
    "        self.prediction = prediction\n",
    "        self.correct_label = correct_label\n",
    "        \n",
    "    def __lt__(self, p1):\n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 < prob2\n",
    "    \n",
    "    def __le__(self, p1):\n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 <= prob2\n",
    "    \n",
    "    def __eq__(self, p1): \n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 == prob2\n",
    "    \n",
    "    def __ne__(self, p1): \n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 != prob2\n",
    "    \n",
    "    def __gt__(self, p1): \n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 > prob2\n",
    "    \n",
    "    def __ge__(self, p1): \n",
    "        prob1 = self.prediction.get_stat_for_value(class_label)\n",
    "        prob2 = p1.prediction.get_stat_for_value(class_label)\n",
    "        return prob1 >= prob2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Stats:\n",
    "    def __init__(self, class_label):\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def get_TP_rate(self) -> float:\n",
    "        return self.get_TP() / (self.get_TP() + self.get_FN())\n",
    "\n",
    "    def get_FP_rate(self) -> float:\n",
    "        return self.get_FP() / (self.get_FP() + self.get_TN())\n",
    "\n",
    "    def get_precision(self) -> float:\n",
    "        return self.get_TP() / (self.get_TP() + self.get_FP())\n",
    "\n",
    "    def get_recall(self) -> float:\n",
    "        return self.get_TP() / (self.get_TP() + self.get_FN())\n",
    "\n",
    "    def get_F_measure(self) -> float:\n",
    "        return 2 * (self.getPrecision() * self.getRecall()) / (self.getPrecision() + self.getRecall())\n",
    "\n",
    "    def get_ROC_area(bench: BenchmarkResult) -> float:\n",
    "        result = 0\n",
    "        preds = bench.get_predictions()\n",
    "        \n",
    "        preds = sorted(preds) \n",
    "\n",
    "        n_class_count = 0\n",
    "        y_class_count = 0\n",
    "        uy  = 0\n",
    "        un = 0\n",
    "        for p in preds:\n",
    "            if p.correct_label == class_label:\n",
    "                uy += n_class_count\n",
    "                y_class_count += 1\n",
    "            else:\n",
    "                un += y_class_count\n",
    "                n_class_count += 1\n",
    "\n",
    "        result = uy / (uy + un)\n",
    "        return result\n",
    "\n",
    "    def get_class_label(self) -> str:\n",
    "        return class_label\n",
    "\n",
    "    def set_class_label(class_label: str) -> None:\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def get_TP(self) -> float:\n",
    "        return TP\n",
    "\n",
    "    def set_TP(self, TP: float) -> None:\n",
    "        self.TP = TP\n",
    "\n",
    "    def get_FP(self) -> float:\n",
    "        return FP\n",
    "\n",
    "    def set_FP(self, FP: float) -> None:\n",
    "        self.FP = FP\n",
    "\n",
    "    def get_TN(self) -> float:\n",
    "        return TN\n",
    "\n",
    "    def set_TN(self, TN: float) -> None:\n",
    "        self.TN = TN\n",
    "\n",
    "    def get_FN(self) -> float:\n",
    "        return FN\n",
    "\n",
    "    def set_FN(self, FN: float) -> None:\n",
    "        self.FN = FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-figure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-sherman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-cambodia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testabc:\n",
    "    def __init__(self, prediction: int):\n",
    "        self.prediction = prediction\n",
    "        \n",
    "    def __lt__(self, p1):\n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 < prob2\n",
    "    \n",
    "    def __le__(self, p1):\n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 <= prob2\n",
    "    \n",
    "    def __eq__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 == prob2\n",
    "    \n",
    "    def __ne__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 != prob2\n",
    "    \n",
    "    def __gt__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 > prob2\n",
    "    \n",
    "    def __ge__(self, p1): \n",
    "        prob1 = self.prediction\n",
    "        prob2 = p1.prediction\n",
    "        return prob1 >= prob2\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = Testabc(3)\n",
    "test2 = Testabc(63)\n",
    "test3 = Testabc(33)\n",
    "test4 = Testabc(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Testabc at 0x7f9128073250>,\n",
       " <__main__.Testabc at 0x7f9128073b50>,\n",
       " <__main__.Testabc at 0x7f9128073310>,\n",
       " <__main__.Testabc at 0x7f91280733a0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([test1, test2, test3, test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "33\n",
      "63\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "for a in sorted([test1, test2, test3, test4]):\n",
    "    print(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 <test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-deputy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
