{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp uid3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7eec2",
   "metadata": {},
   "source": [
    "# UId3\n",
    "\n",
    "> Class implementing UId3 algorithm for growing decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from graphviz import Source\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from .attribute import Attribute\n",
    "from .data import Data\n",
    "from .entropy_evaluator import EntropyEvaluator\n",
    "from .tree import Tree\n",
    "from .tree_node import TreeNode\n",
    "from .tree_edge import TreeEdge\n",
    "from .tree_evaluator import TreeEvaluator\n",
    "from .value import Value\n",
    "from .reading import Reading\n",
    "from .instance import Instance\n",
    "from multiprocessing import cpu_count,Pool\n",
    "import math\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UId3(BaseEstimator):\n",
    "    \n",
    "    PARALLEL_ENTRY_FACTOR = 1000\n",
    "\n",
    "    def __init__(self, max_depth=2, node_size_limit = 1, grow_confidence_threshold = 0, min_impurity_decrease=0):\n",
    "        self.TREE_DEPTH_LIMIT= max_depth\n",
    "        self.NODE_SIZE_LIMIT = node_size_limit\n",
    "        self.GROW_CONFIDENCE_THRESHOLD = grow_confidence_threshold\n",
    "        self.tree = None\n",
    "        self.node_size_limit = node_size_limit\n",
    "        self.min_impurity_decrease=min_impurity_decrease\n",
    "        \n",
    "    def fit(self, data, y=None, *, depth,  entropyEvaluator, classifier=None, beta=1, discount_importance = False, n_jobs=None): \n",
    "        \"\"\"Fits pyUID3 tree, optionally using SHAP values calculated for the classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pyuid3.Data\n",
    "            Data object containing dataset. It has to be object from pyuid3.Data\n",
    "        y : np.array\n",
    "            Vector containing target values\n",
    "        depth : int, optional\n",
    "            This parameter should not be used. It is used internally by recurrent calls to govern the depth of the tree.\n",
    "        entropyEvaluator: pyuid3.EntropyEvaluator\n",
    "            Object responisble for calculating split criterion. Default is UncertainEntropyEvaluator. Although the naming might be confusing, other possibilities are:\n",
    "            UncertainGiniEvaluator, UncertainSqrtGiniEvaluator\n",
    "        classifier: optional\n",
    "            A classifier that is designed according to sckit paradigm. It is required from the classifier to have predict_proba function. Default is None\n",
    "        beta: int\n",
    "            Parameter being a weight in harmonic mean between score obtained from EntropyEvaluator and SHAP values. \n",
    "            The greater the value the more important are SHAP values when selecting a split. Default is 1.\n",
    "        discount_importance: boolean,\n",
    "            Parameter indicating if the SHAP importances should be calculated resively at every split, or if the importances calculated for the whole data should be used.\n",
    "            In the latter case, the importances are discounted by the percentage of reduction in split criterion (e.g. Information Gain). Default it False.\n",
    "        n_jobs: int, optional\n",
    "            Number of processess to use when building a tree. Default is None\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pyuid3.Tree\n",
    "            a fitted decision tree\n",
    "        \"\"\"\n",
    "\n",
    "        if classifier is not None and len(data.get_instances()) >= self.NODE_SIZE_LIMIT:\n",
    "            datadf = data.to_dataframe()\n",
    "            try:\n",
    "                explainer = shap.Explainer(classifier,datadf.iloc[:,:-1])\n",
    "                if hasattr(explainer, \"shap_values\"):\n",
    "                    shap_values = explainer.shap_values(datadf.iloc[:,:-1],check_additivity=False)\n",
    "                else:\n",
    "                    shap_values = explainer(datadf.iloc[:,:-1]).values\n",
    "                    shap_values=[sv for sv in np.moveaxis(shap_values, 2,0)]\n",
    "                if hasattr(explainer, \"expected_value\"):\n",
    "                    expected_values = explainer.expected_value\n",
    "                else:\n",
    "                    expected_values=[np.mean(v) for v in shap_values]\n",
    "            except TypeError:\n",
    "                explainer = shap.Explainer(classifier.predict_proba, datadf.iloc[:,:-1])\n",
    "                shap_values = explainer(datadf.iloc[:,:-1]).values\n",
    "                shap_values=[sv for sv in np.moveaxis(shap_values, 2,0)]\n",
    "                expected_values=[np.mean(v) for v in shap_values]\n",
    "            \n",
    "            \n",
    "            if type(shap_values) is not list:\n",
    "                raise ValueError(\"\"\"Dimensions of SHAP values is incorrect. It should be equal to number of classess in classification problem. \n",
    "                                 It might be caused by usage of XGBClassifier, which cannot properly claclate expected values and importances with SHAP. \n",
    "                                 See https://github.com/slundberg/shap/issues/352 for details.\"\"\")\n",
    "\n",
    "            #find max and rescale:\n",
    "            #maxshap = max([np.max(np.abs(sv)) for sv in shap_values]) #ADD\n",
    "            #shap_values = [sv/maxshap for sv in shap_values]#ADD\n",
    "            \n",
    "            shap_dict={}\n",
    "            expected_dict={}\n",
    "            for i,v in enumerate(shap_values):\n",
    "                shap_dict[str(i)] = pd.DataFrame(v, columns = datadf.columns[:-1])\n",
    "                expected_dict[str(i)] = expected_values[i] #/maxshap #ADD\n",
    "                \n",
    "            data = data.set_importances(pd.concat(shap_dict,axis=1), expected_values = expected_dict)\n",
    "        \n",
    "        if len(data.get_instances()) < self.NODE_SIZE_LIMIT:\n",
    "            return None\n",
    "        if depth > self.TREE_DEPTH_LIMIT:\n",
    "            return None\n",
    "        entropy = entropyEvaluator.calculate_entropy(data)\n",
    "\n",
    "        data.update_attribute_domains()\n",
    "\n",
    "        # of the set is heterogeneous or no attributes to split, just class -- return\n",
    "        # leaf\n",
    "        if entropy == 0 or len(data.get_attributes()) == 1:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        \n",
    "        cl=[]\n",
    "        for i in data.get_instances():\n",
    "            cl.append(i.get_reading_for_attribute(data.get_class_attribute()).get_most_probable().get_name())\n",
    "\n",
    "        n_jobs_inner = 1\n",
    "        if n_jobs is not None:\n",
    "            if n_jobs == -1:\n",
    "                n_jobs=n_jobs_inner = cpu_count()\n",
    "            if len(data)/n_jobs_inner < UId3.PARALLEL_ENTRY_FACTOR:\n",
    "                n_jobs_inner = max(1,int(len(data)/UId3.PARALLEL_ENTRY_FACTOR)) \n",
    "            if n_jobs > len(data.get_attributes()):\n",
    "                n_jobs = len(data.get_attributes())-1\n",
    "            if (len(data)*len(data.get_attributes()))/n_jobs < UId3.PARALLEL_ENTRY_FACTOR:\n",
    "                n_jobs = max(1,int((len(data)*len(data.get_attributes()))/UId3.PARALLEL_ENTRY_FACTOR))\n",
    "        else:\n",
    "            n_jobs = 1\n",
    "            \n",
    "\n",
    "        if n_jobs > 1 and n_jobs_inner < len(data.get_attributes()):\n",
    "            with Pool(n_jobs) as pool:\n",
    "                results = pool.starmap(UId3.try_attribute_for_split, [(data, a, cl, entropy,  entropyEvaluator,self.min_impurity_decrease, beta, 1,classifier is not None) for a in data.get_attributes() if a != data.get_class_attribute()])\n",
    "                temp_gain = 0\n",
    "                for temp_gain, pure_temp_gain, best_split_candidate in results:\n",
    "                    if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:\n",
    "                        info_gain = temp_gain\n",
    "                        pure_info_gain=pure_temp_gain\n",
    "                        best_split = best_split_candidate\n",
    "        else:\n",
    "            for a in data.get_attributes():\n",
    "                if data.get_class_attribute() == a:\n",
    "                    continue\n",
    "                temp_gain, pure_temp_gain, best_split_candidate=self.try_attribute_for_split(data, a, cl, entropy,  entropyEvaluator,self.min_impurity_decrease, beta=beta, n_jobs=n_jobs, shap = classifier is not None)\n",
    "                if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:\n",
    "                    info_gain = temp_gain\n",
    "                    pure_info_gain=pure_temp_gain\n",
    "                    best_split = best_split_candidate\n",
    "\n",
    "        \n",
    "\n",
    "        # if nothing better can happen\n",
    "        if best_split == None:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "\n",
    "        # Create root node, and recursively go deeper into the tree.\n",
    "        class_att = data.get_class_attribute()\n",
    "        class_stats = data.calculate_statistics(class_att)\n",
    "        root = TreeNode(best_split.get_name(), class_stats)\n",
    "        root.set_type(class_att.get_type())\n",
    "        \n",
    "        #TODO: get best split, check if depth > 0, fit SVM with this two (SVM, due to small number of data), filter subsets based on the decision boundary, save model for predict\n",
    "        \n",
    "\n",
    "        # attach newly created trees\n",
    "        for val in best_split.get_splittable_domain():\n",
    "            if best_split.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data = data.filter_nominal_attribute_value(best_split, val)\n",
    "                if not discount_importance:\n",
    "                    subtree = self.fit(new_data, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                else:\n",
    "                    new_data = new_data.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                    subtree = self.fit(new_data, discount_importance=True, classifier=None, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                \n",
    "                if subtree and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                    root.add_edge(TreeEdge(Value(val, best_split_stats.get_avg_confidence()), subtree.get_root()))\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "            elif best_split.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data_less_then,new_data_greater_equal = data.filter_numeric_attribute_value(best_split, val)\n",
    "                \n",
    "                \n",
    "                if len(new_data_less_then) >= self.node_size_limit and len(new_data_greater_equal) >= self.node_size_limit:\n",
    "                    if not discount_importance:\n",
    "                        subtree_less_than = self.fit(new_data_less_then, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                        subtree_greater_equal = self.fit(new_data_greater_equal, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                    else:\n",
    "                        new_data_less_then = new_data_less_then.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                        new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                        subtree_less_than = self.fit(new_data_less_then, classifier=None,  entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=True,beta=beta, n_jobs=n_jobs)\n",
    "                        subtree_greater_equal = self.fit(new_data_greater_equal, classifier=None, entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=True,beta=beta, n_jobs=n_jobs)\n",
    "\n",
    "                    if subtree_less_than and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\"<\" + val, best_split_stats.get_avg_confidence()), subtree_less_than.get_root()))\n",
    "                    if subtree_greater_equal and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\">=\" + val, best_split_stats.get_avg_confidence()), subtree_greater_equal.get_root()))\n",
    "                    root.set_type(Attribute.TYPE_NUMERICAL)\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "        if len(root.get_edges()) == 0:\n",
    "            root.set_att(data.get_class_attribute().get_name())\n",
    "            root.set_type(data.get_class_attribute().get_type())\n",
    "\n",
    "        self.tree = Tree(root)\n",
    "        return self.tree\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def try_attribute_for_split(data, attribute, cl, globalEntropy, entropyEvaluator,min_impurity_decrease, beta=1, n_jobs=None, shap=False):\n",
    "        values = attribute.get_domain()\n",
    "        pure_info_gain = 0\n",
    "        info_gain=0\n",
    "        best_split=None\n",
    "        \n",
    "        stats = data.calculate_statistics(attribute)\n",
    "        \n",
    "        ## start searching for best border values  -- such that class value remains the same for the ranges between them\n",
    "        if attribute.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "            border_search_list = []\n",
    "            for i in data.get_instances():\n",
    "                v=i.get_reading_for_attribute(attribute).get_most_probable().get_name()\n",
    "                border_search_list.append([v])\n",
    "            border_search_df = pd.DataFrame(border_search_list,columns=['values'])\n",
    "            border_search_df['values']=border_search_df['values'].astype('f8')\n",
    "            border_search_df['class'] = cl\n",
    "            border_search_df=border_search_df.sort_values(by='values')\n",
    "            border_search_df['values_shift']=border_search_df['values'].shift(1)\n",
    "            border_search_df['class_shitf'] = border_search_df['class'].shift(1)\n",
    "            border_search_shift = border_search_df[border_search_df['class_shitf'] != border_search_df['class']]\n",
    "            values = np.unique((border_search_shift['values']+border_search_shift['values_shift']).dropna()/2).astype('str') # take the middle value \n",
    "        else:\n",
    "            values=list(values)\n",
    "\n",
    "        if n_jobs is not None and attribute.get_type()==Attribute.TYPE_NUMERICAL: \n",
    "            if n_jobs == -1:\n",
    "                n_jobs = cpu_count()\n",
    "            if len(values)/n_jobs < UId3.PARALLEL_ENTRY_FACTOR:\n",
    "                n_jobs = max(1,int(len(values)/UId3.PARALLEL_ENTRY_FACTOR))\n",
    "        else:\n",
    "            n_jobs = 1\n",
    "\n",
    "        #divide into j_jobs batches\n",
    "        if n_jobs > 1:\n",
    "            values_batches = np.array_split(values, n_jobs)\n",
    "            with Pool(n_jobs) as pool:\n",
    "                results = pool.starmap(UId3.calculate_split_criterion, [(v, data, attribute, stats, globalEntropy, entropyEvaluator, min_impurity_decrease,beta,shap) for v in values_batches])\n",
    "                temp_gain = 0\n",
    "                for best_split_candidate_c, value_to_split_on_c, temp_gain_c, pure_temp_gain_c in results:\n",
    "                    if temp_gain_c > temp_gain:\n",
    "                        best_split_candidate=best_split_candidate_c \n",
    "                        value_to_split_on =value_to_split_on_c\n",
    "                        temp_gain =temp_gain_c\n",
    "                        pure_temp_gain=pure_temp_gain_c\n",
    "        else:\n",
    "            best_split_candidate, value_to_split_on, temp_gain, pure_temp_gain = UId3.calculate_split_criterion(values=values, \n",
    "                                                                                                                data=data, \n",
    "                                                                                                                attribute=attribute, \n",
    "                                                                                                                stats=stats, \n",
    "                                                                                                                globalEntropy=globalEntropy, \n",
    "                                                                                                                entropyEvaluator=entropyEvaluator, \n",
    "                                                                                                                min_impurity_decrease=min_impurity_decrease,\n",
    "                                                                                                                beta=beta,shap=shap)\n",
    "\n",
    "\n",
    "        if temp_gain > info_gain and (pure_temp_gain/globalEntropy)>=min_impurity_decrease:\n",
    "            info_gain = temp_gain\n",
    "            pure_info_gain=pure_temp_gain\n",
    "            best_split = best_split_candidate\n",
    "            best_split_candidate.set_importance_gain(pure_info_gain)\n",
    "            best_split_candidate.set_value_to_split_on(value_to_split_on)\n",
    "            \n",
    "        return info_gain, pure_info_gain, best_split\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_split_criterion( values, data, attribute, stats, globalEntropy, entropyEvaluator,min_impurity_decrease, beta=1, shap=False):\n",
    "        temp_gain = 0\n",
    "        temp_shapgain = 0\n",
    "        temp_numeric_gain = 0\n",
    "        pure_temp_gain=0\n",
    "        local_info_gain = 0\n",
    "        value_to_split_on = None\n",
    "        best_split = None\n",
    "        \n",
    "        def get_maximum_label(shapdict):\n",
    "            return max(shapdict, key=shapdict.get)\n",
    "        \n",
    "        def get_shap_stats(data, alignment=True):\n",
    "            labels = [get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in data.instances]\n",
    "            if alignment:\n",
    "                true_labels = [i.get_reading_for_attribute(data.get_class_attribute().get_name()).get_most_probable().get_name() for i in data.instances]\n",
    "                shap_align = 0 if len(labels)== 0 else (np.array(labels)==np.array(true_labels)).sum()/len(labels)\n",
    "            else:\n",
    "                shap_align=1\n",
    "            return shap_align,entropyEvaluator.calculate_raw_entropy(labels)\n",
    "        \n",
    "        if shap:\n",
    "            global_shap_align,globalShaptropy = get_shap_stats(data, alignment=False)\n",
    "            globalShaptropy += (1-global_shap_align) \n",
    "        \n",
    "        for v in values:  \n",
    "            subdata = None\n",
    "            subdataLessThan = None\n",
    "            subdataGreaterEqual = None\n",
    "            if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                subdata = data.filter_nominal_attribute_value(attribute, v)\n",
    "            elif attribute.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                subdata_less_than,subdata_greater_equal = data.filter_numeric_attribute_value(attribute, v)\n",
    "                \n",
    "            if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                stat_for_value = len(subdata)/len(data)\n",
    "                temp_gain += (stat_for_value) * entropyEvaluator.calculate_entropy(subdata)\n",
    "                if shap:\n",
    "                    shap_align_subdata,shaptropy_subdata = get_shap_stats(subdata, alignment=False)\n",
    "                    temp_shapgain += (stat_for_value) * (shaptropy_subdata + (1-shap_align_subdata)) \n",
    "            elif attribute.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                stat_for_lt_value = len(subdata_less_than)/len(data)\n",
    "                stat_for_gte_value = len(subdata_greater_equal)/len(data)\n",
    "                conf_for_value = stats.get_avg_confidence()\n",
    "                avg_abs_importance = stats.get_avg_abs_importance()\n",
    "                if shap:\n",
    "                \n",
    "                    shap_align_lt,shaptropy_lt = get_shap_stats(subdata_less_than, alignment=False)\n",
    "                    shap_align_gte,shaptropy_gte = get_shap_stats(subdata_greater_equal, alignment=False)\n",
    "\n",
    "                    pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+\n",
    "                                                                                   (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))\n",
    "                    \n",
    "                    shap_align_lt=shap_align_gte=1\n",
    "                    \n",
    "\n",
    "                    pure_single_temp_gain_shap = (globalShaptropy   -(stat_for_lt_value*(shaptropy_lt+(1-shap_align_lt))+stat_for_gte_value*(shaptropy_gte+(1-shap_align_gte))))*avg_abs_importance \n",
    "                   \n",
    "                    if pure_single_temp_gain*pure_single_temp_gain_shap == 0:\n",
    "                        #to prevent from 0-division\n",
    "                        single_temp_gain=0\n",
    "                    else:\n",
    "                        single_temp_gain =((1+beta**2)*pure_single_temp_gain_shap*pure_single_temp_gain)/((beta**2*pure_single_temp_gain_shap)+pure_single_temp_gain)*conf_for_value\n",
    "                else:\n",
    "                    pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+\n",
    "                                                                                           (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))\n",
    "                    single_temp_gain = pure_single_temp_gain*conf_for_value\n",
    "                    \n",
    "                    \n",
    "                if single_temp_gain > temp_numeric_gain:\n",
    "                    temp_numeric_gain = single_temp_gain\n",
    "                    temp_gain = single_temp_gain\n",
    "                    pure_temp_gain= pure_single_temp_gain\n",
    "                    value_to_split_on = v\n",
    "                    \n",
    "        if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "            conf_for_value = stats.get_avg_confidence()\n",
    "            pure_temp_gain=globalEntropy-temp_gain\n",
    "            if shap:\n",
    "                avg_abs_importance = stats.get_avg_abs_importance()\n",
    "                pure_temp_gain_shap = (globalShaptropy-temp_shapgain)*avg_abs_importance\n",
    "                temp_gain = ((1+beta**2)*pure_temp_gain_shap*pure_temp_gain)/((beta**2*pure_temp_gain_shap)+pure_temp_gain)*conf_for_value\n",
    "            else:\n",
    "                temp_gain = conf_for_value*pure_temp_gain\n",
    "\n",
    "        if temp_gain > local_info_gain and (pure_temp_gain/globalEntropy)>=min_impurity_decrease:\n",
    "            best_split = attribute\n",
    "            local_info_gain=temp_gain\n",
    " \n",
    "        return best_split, value_to_split_on, temp_gain, pure_temp_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_uncertain_nominal() -> None:\n",
    "        data = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "        test = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "\n",
    "        t = UId3.fit(data, UncertainEntropyEvaluator(), 0)\n",
    "        br = TreeEvaluator.train_and_test(t, test)\n",
    "\n",
    "        print(\"###############################################################\")\n",
    "        print(f\"Correctly classified instances: {br.get_accuracy() * 100}%\")\n",
    "        print(f\"Incorrectly classified instances: {(1-br.get_accuracy()) * 100}%\")\n",
    "        print(\"TP Rate\", \"FP Rate\", \"Precision\", \"Recall\", \"F-Measure\", \"ROC Area\", \"Class\")\n",
    "\n",
    "        for class_label in data.get_class_attribute().get_domain():\n",
    "            cs = br.get_stats_for_label(class_label)\n",
    "            print(cs.get_TP_rate(), cs.get_FP_rate(), cs.get_precision(), cs.get_recall(), cs.get_F_measure(),\n",
    "                                cs.get_ROC_area(br), cs.get_class_label())\n",
    "\n",
    "    def predict(self, X):   # should take array-like X -> predict(X)\n",
    "        if not isinstance(X, (list, np.ndarray)):\n",
    "            X = [X]\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            att_stats = self.tree.predict(instance)\n",
    "            predictions.append(att_stats.get_most_probable())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cce47f",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d6929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if outlook (play= {no[0.36],yes[0.64]})\n",
      "|is  overcast[0.94] then play = {yes[1.0],no[0.0]} \n",
      "|is  sunny[0.94]then if humidity (play= {yes[0.5],no[0.5]} )\n",
      "|-------is  normal[0.93] then play = {yes[1.0],no[0.0]} \n",
      "|-------is  high[0.93] then play = {yes[0.4],no[0.6]} \n",
      "|is  rainy[0.94]then if windy (play= {no[0.4],yes[0.6]} )\n",
      "|-------is  TRUE[0.94] then play = {no[0.67],yes[0.33]} \n",
      "|-------is  FALSE[0.94] then play = {yes[1.0],no[0.0]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#instance = data.instances[0]   # powinno zwrocic pierwszy wiersz \"tabeli\"\n",
    "type_t=\"str\"\n",
    "data = Data.parse_uarff(\"../resources/weather.nominal.uncertain.arff\")\n",
    "\n",
    "uid3 = UId3()\n",
    "t = uid3.fit(data, depth=0, entropyEvaluator=UncertainEntropyEvaluator())\n",
    "if type_t == \"dot\":\n",
    "    result = t.to_dot()\n",
    "    print(result)\n",
    "    s = Source(result, filename=\"test.gv\", format=\"png\")\n",
    "    s.view()\n",
    "elif type_t == \"str\": \n",
    "    print(str(t))\n",
    "elif type_t == \"hmr\":\n",
    "    print(t.to_HMR())\n",
    "elif type_t == \"infogain\":\n",
    "    print(t.get_importances())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
