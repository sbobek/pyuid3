{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp uid3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7eec2",
   "metadata": {},
   "source": [
    "# UId3\n",
    "\n",
    "> Class implementing UId3 algorithm for growing decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from graphviz import Source\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyuid3.attribute import Attribute\n",
    "from pyuid3.data import Data\n",
    "from pyuid3.entropy_evaluator import EntropyEvaluator\n",
    "from pyuid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator\n",
    "from pyuid3.tree import Tree\n",
    "from pyuid3.tree_node import TreeNode\n",
    "from pyuid3.tree_edge import TreeEdge\n",
    "from pyuid3.tree_evaluator import TreeEvaluator\n",
    "from pyuid3.value import Value\n",
    "from pyuid3.reading import Reading\n",
    "from pyuid3.instance import Instance\n",
    "from multiprocessing import cpu_count,Pool\n",
    "import math\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UId3(BaseEstimator):\n",
    "    \n",
    "    PARALLEL_ENTRY_FACTOR = 10 #ten times as much data as there are cores on the machine\n",
    "\n",
    "    def __init__(self, max_depth=2, node_size_limit = 1, grow_confidence_threshold = 0, min_impurity_decrease=0):\n",
    "        self.TREE_DEPTH_LIMIT= max_depth\n",
    "        self.NODE_SIZE_LIMIT = node_size_limit\n",
    "        self.GROW_CONFIDENCE_THRESHOLD = grow_confidence_threshold\n",
    "        self.tree = None\n",
    "        self.node_size_limit = node_size_limit\n",
    "        self.min_impurity_decrease=min_impurity_decrease\n",
    "        \n",
    "    def fitshap(self, data, y=None, *, depth,  entropyEvaluator, classifier , beta=1, n_jobs=None): \n",
    "        #calculate shap importances for data (data-> to_dataframe)\n",
    "        #update shap importances for data  (or not -- to make it faster, simply keep it in separate dataframe) how filtering?\n",
    "        #look-ahead? What is happening with shap after split? -- this will be overkill!\n",
    "        if classifier is not None and len(data.get_instances()) >= self.NODE_SIZE_LIMIT:\n",
    "            datadf = data.to_dataframe()\n",
    "            explainer = shap.Explainer(classifier,datadf.iloc[:,:-1],model_output='probability')\n",
    "            shap_values = explainer.shap_values(datadf.iloc[:,:-1],check_additivity=True)\n",
    "            if type(shap_values) is not list:\n",
    "                raise ValueError(\"\"\"Dimensions of SHAP values is incorrect. It should be equal to number of classess in classification problem. \n",
    "                                 It might be caused by usage of XGBClassifier, which cannot properly claclate expected values and importances with SHAP. \n",
    "                                 See https://github.com/slundberg/shap/issues/352 for details.\"\"\")\n",
    "        \n",
    "            shap_dict={}\n",
    "            expected_dict={}\n",
    "            expected_values = explainer.expected_value\n",
    "            for i,v in enumerate(shap_values):\n",
    "                shap_dict[str(i)] = pd.DataFrame(v, columns = datadf.columns[:-1])\n",
    "                expected_dict[str(i)] = expected_values[i]\n",
    "                \n",
    "            data = data.set_importances(pd.concat(shap_dict,axis=1), expected_values = expected_dict)\n",
    "        \n",
    "        if len(data.get_instances()) < self.NODE_SIZE_LIMIT:\n",
    "            return None\n",
    "        if depth > self.TREE_DEPTH_LIMIT:\n",
    "            return None\n",
    "        entropy = entropyEvaluator.calculate_entropy(data)\n",
    "\n",
    "        data.update_attribute_domains()\n",
    "\n",
    "        # of the set is heterogeneous or no attributes to split, just class -- return\n",
    "        # leaf\n",
    "        if entropy == 0 or len(data.get_attributes()) == 1:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        \n",
    "        \n",
    "        cl=[]\n",
    "        for i in data.get_instances():\n",
    "            cl.append(i.get_reading_for_attribute(data.get_class_attribute()).get_most_probable().get_name())\n",
    "        \n",
    "        for a in data.get_attributes():\n",
    "            if data.get_class_attribute() == a:\n",
    "                continue\n",
    "                \n",
    "            values = a.get_domain()\n",
    "            pure_info_gain = 0\n",
    "            stats = data.calculate_statistics(a)\n",
    "            \n",
    "            ## start searching for best border values  -- such that class value remains the same for the ranges between them\n",
    "            if a.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                border_search_list = []\n",
    "                for i in data.get_instances():\n",
    "                    v=i.get_reading_for_attribute(a).get_most_probable().get_name()\n",
    "                    border_search_list.append([v])\n",
    "                # border_search_df = pd.DataFrame(border_search_list,columns=['values'])\n",
    "                # border_search_df['values']=border_search_df['values'].astype('f8')\n",
    "                # border_search_df=border_search_df.sort_values(by='values')\n",
    "                # border_search_df['values_shift']=border_search_df['values'].shift(1)\n",
    "                # border_search_shift = border_search_df\n",
    "                # values = np.unique((border_search_shift['values']+border_search_shift['values_shift']).dropna()/2).astype('str') # take the middle value \n",
    "                border_search_df = pd.DataFrame(border_search_list,columns=['values'])\n",
    "                border_search_df['values']=border_search_df['values'].astype('f8')\n",
    "                border_search_df['class'] = cl\n",
    "                border_search_df=border_search_df.sort_values(by='values')\n",
    "                border_search_df['values_shift']=border_search_df['values'].shift(1)\n",
    "                border_search_df['class_shitf'] = border_search_df['class'].shift(1)\n",
    "                border_search_shift = border_search_df[border_search_df['class_shitf'] != border_search_df['class']]\n",
    "                values = np.unique((border_search_shift['values']+border_search_shift['values_shift']).dropna()/2).astype('str') # take the middle value \n",
    "                \n",
    "                if n_jobs is not None:\n",
    "                    if n_jobs == -1:\n",
    "                        n_jobs = cpu_count()\n",
    "                    if len(values)/n_jobs < self.PARALLEL_ENTRY_FACTOR:\n",
    "                        n_jobs = max(1,int(len(values)/self.PARALLEL_ENTRY_FACTOR))\n",
    "                else:\n",
    "                    n_jobs = 1\n",
    "                        \n",
    "                #divide into j_jobs batches\n",
    "                if n_jobs > 1:\n",
    "                    values_batches = np.array_split(values, n_jobs)\n",
    "                    with Pool(n_jobs) as pool:\n",
    "                        results = pool.starmap(self.calculate_split_criterion, [(v, data, a, stats, entropy, entropyEvaluator, self.min_impurity_decrease,beta,True) for v in values_batches])\n",
    "                        temp_gain = 0\n",
    "                        for best_split_candidate_c, value_to_split_on_c, temp_gain_c, pure_temp_gain_c in results:\n",
    "                            if temp_gain_c > temp_gain:\n",
    "                                best_split_candidate=best_split_candidate_c \n",
    "                                value_to_split_on =value_to_split_on_c\n",
    "                                temp_gain =temp_gain_c\n",
    "                                pure_temp_gain=pure_temp_gain_c\n",
    "                else:\n",
    "                    best_split_candidate, value_to_split_on, temp_gain, pure_temp_gain = self.calculate_split_criterion(values=values, \n",
    "                                                                                                                        data=data, \n",
    "                                                                                                                        attribute=a, \n",
    "                                                                                                                        stats=stats, \n",
    "                                                                                                                        globalEntropy=entropy, \n",
    "                                                                                                                        entropyEvaluator=entropyEvaluator, \n",
    "                                                                                                                        min_impurity_decrease=self.min_impurity_decrease,\n",
    "                                                                                                                        beta=beta,shap=True)\n",
    "                    \n",
    "\n",
    "            #this was move out from the loop to reduce numerical errors while iteratively sum and divide\n",
    "            if a.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                conf_for_value = stats.get_avg_confidence()\n",
    "                pure_temp_gain=entropy-temp_gain\n",
    "                temp_gain = conf_for_value*pure_temp_gain\n",
    "            if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:\n",
    "                info_gain = temp_gain\n",
    "                pure_info_gain=pure_temp_gain\n",
    "                best_split = best_split_candidate\n",
    "                best_split_candidate.set_importance_gain(pure_info_gain)\n",
    "                best_split_candidate.set_value_to_split_on(value_to_split_on)\n",
    "\n",
    "        # if nothing better can happen\n",
    "        if best_split == None:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "        # Create root node, and recursively go deeper into the tree.\n",
    "        class_att = data.get_class_attribute()\n",
    "        class_stats = data.calculate_statistics(class_att)\n",
    "        root = TreeNode(best_split.get_name(), class_stats)\n",
    "        root.set_type(class_att.get_type())\n",
    "\n",
    "        # attach newly created trees\n",
    "        for val in best_split.get_splittable_domain():\n",
    "            if best_split.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data = data.filter_nominal_attribute_value(best_split, val)\n",
    "\n",
    "                subtree = self.fitshap(new_data, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                \n",
    "                if subtree and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                    root.add_edge(TreeEdge(Value(val, best_split_stats.get_avg_confidence()), subtree.get_root()))\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "            elif best_split.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data_less_then,new_data_greater_equal = data.filter_numeric_attribute_value(best_split, val)\n",
    "                \n",
    "                \n",
    "                if len(new_data_less_then) >= self.node_size_limit and len(new_data_greater_equal) >= self.node_size_limit:\n",
    "\n",
    "                    subtree_less_than = self.fitshap(new_data_less_then, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "                    subtree_greater_equal = self.fitshap(new_data_greater_equal, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, n_jobs=n_jobs)\n",
    "\n",
    "                    if subtree_less_than and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\"<\" + val, best_split_stats.get_avg_confidence()), subtree_less_than.get_root()))\n",
    "                    if subtree_greater_equal and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\">=\" + val, best_split_stats.get_avg_confidence()), subtree_greater_equal.get_root()))\n",
    "                    root.set_type(Attribute.TYPE_NUMERICAL)\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "        if len(root.get_edges()) == 0:\n",
    "            root.set_att(data.get_class_attribute().get_name())\n",
    "            root.set_type(data.get_class_attribute().get_type())\n",
    "\n",
    "        self.tree = Tree(root)\n",
    "        return self.tree\n",
    "\n",
    "    def fit(self, data, y=None, *, depth,  entropyEvaluator, discount_importance = False,beta=1, n_jobs=None):   # data should be split into array-like X and y and then fit should be 'fit(X, y)':\n",
    "\n",
    "        if len(data.get_instances()) < self.NODE_SIZE_LIMIT:\n",
    "            return None\n",
    "        if depth > self.TREE_DEPTH_LIMIT:\n",
    "            return None\n",
    "        entropy = entropyEvaluator.calculate_entropy(data)\n",
    "\n",
    "        data.update_attribute_domains()\n",
    "\n",
    "        # of the set is heterogeneous or no attributes to split, just class -- return\n",
    "        # leaf\n",
    "        if entropy == 0 or len(data.get_attributes()) == 1:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        \n",
    "        \n",
    "        cl=[]\n",
    "        for i in data.get_instances():\n",
    "            cl.append(i.get_reading_for_attribute(data.get_class_attribute()).get_most_probable().get_name())\n",
    "        \n",
    "        for a in data.get_attributes():\n",
    "            if data.get_class_attribute() == a:\n",
    "                continue\n",
    "                \n",
    "            values = a.get_domain()\n",
    "            pure_info_gain = 0\n",
    "            stats = data.calculate_statistics(a)\n",
    "            \n",
    "            ## start searching for best border values  -- such that class value remains the same for the ranges between them\n",
    "            if a.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                border_search_list = []\n",
    "                for i in data.get_instances():\n",
    "                    v=i.get_reading_for_attribute(a).get_most_probable().get_name()\n",
    "                    border_search_list.append([v])\n",
    "                border_search_df = pd.DataFrame(border_search_list,columns=['values'])\n",
    "                border_search_df['values']=border_search_df['values'].astype('f8')\n",
    "                border_search_df['class'] = cl\n",
    "                border_search_df=border_search_df.sort_values(by='values')\n",
    "                border_search_df['values_shift']=border_search_df['values'].shift(1)\n",
    "                border_search_df['class_shitf'] = border_search_df['class'].shift(1)\n",
    "                border_search_shift = border_search_df[border_search_df['class_shitf'] != border_search_df['class']]\n",
    "                values = np.unique((border_search_shift['values']+border_search_shift['values_shift']).dropna()/2).astype('str') # take the middle value \n",
    "                \n",
    "                if n_jobs is not None:\n",
    "                    if n_jobs == -1:\n",
    "                        n_jobs = cpu_count()\n",
    "                    if len(values)/n_jobs < self.PARALLEL_ENTRY_FACTOR:\n",
    "                        n_jobs = max(1,int(len(values)/self.PARALLEL_ENTRY_FACTOR))\n",
    "                else:\n",
    "                    n_jobs = 1\n",
    "                        \n",
    "                #divide into j_jobs batches\n",
    "                if n_jobs > 1:\n",
    "                    values_batches = np.array_split(values, n_jobs)\n",
    "                    with Pool(n_jobs) as pool:\n",
    "                        results = pool.starmap(self.calculate_split_criterion, [(v, data, a, stats, entropy, entropyEvaluator, self.min_impurity_decrease,beta) for v in values_batches])\n",
    "                        temp_gain = 0\n",
    "                        for best_split_candidate_c, value_to_split_on_c, temp_gain_c, pure_temp_gain_c in results:\n",
    "                            if temp_gain_c > temp_gain:\n",
    "                                best_split_candidate=best_split_candidate_c \n",
    "                                value_to_split_on =value_to_split_on_c\n",
    "                                temp_gain =temp_gain_c\n",
    "                                pure_temp_gain=pure_temp_gain_c\n",
    "                else:\n",
    "                    best_split_candidate, value_to_split_on, temp_gain, pure_temp_gain = self.calculate_split_criterion(values=values, \n",
    "                                                                                                                        data=data, \n",
    "                                                                                                                        attribute=a, \n",
    "                                                                                                                        stats=stats, \n",
    "                                                                                                                        globalEntropy=entropy, \n",
    "                                                                                                                        entropyEvaluator=entropyEvaluator, \n",
    "                                                                                                                        min_impurity_decrease=self.min_impurity_decrease,\n",
    "                                                                                                                        beta=beta)\n",
    "                    \n",
    "\n",
    "            #this was move out from the loop to reduce numerical errors while iteratively sum and divide\n",
    "            if a.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                conf_for_value = stats.get_avg_confidence()\n",
    "                pure_temp_gain=entropy-temp_gain\n",
    "                temp_gain = conf_for_value*pure_temp_gain\n",
    "            if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:\n",
    "                info_gain = temp_gain\n",
    "                pure_info_gain=pure_temp_gain\n",
    "                best_split = best_split_candidate\n",
    "                best_split_candidate.set_importance_gain(pure_info_gain)\n",
    "                best_split_candidate.set_value_to_split_on(value_to_split_on)\n",
    "\n",
    "        # if nothing better can happen\n",
    "        if best_split == None:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            tree = Tree(root)\n",
    "            if depth == 0:\n",
    "                self.tree = tree\n",
    "            return tree\n",
    "        # Create root node, and recursively go deeper into the tree.\n",
    "        class_att = data.get_class_attribute()\n",
    "        class_stats = data.calculate_statistics(class_att)\n",
    "        root = TreeNode(best_split.get_name(), class_stats)\n",
    "        root.set_type(class_att.get_type())\n",
    "\n",
    "        # attach newly created trees\n",
    "        for val in best_split.get_splittable_domain():\n",
    "            if best_split.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data = data.filter_nominal_attribute_value(best_split, val)\n",
    "\n",
    "                if discount_importance:\n",
    "                    new_data = new_data.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "\n",
    "                subtree = self.fit(new_data, entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=discount_importance, beta=beta, n_jobs=n_jobs)\n",
    "                \n",
    "                if subtree and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                    root.add_edge(TreeEdge(Value(val, best_split_stats.get_avg_confidence()), subtree.get_root()))\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "            elif best_split.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data_less_then,new_data_greater_equal = data.filter_numeric_attribute_value(best_split, val)\n",
    "                \n",
    "                if discount_importance:\n",
    "                    new_data_less_then = new_data_less_then.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                    new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                \n",
    "                if len(new_data_less_then) >= self.node_size_limit and len(new_data_greater_equal) >= self.node_size_limit:\n",
    "\n",
    "                    subtree_less_than = self.fit(new_data_less_then, entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=discount_importance,beta=beta, n_jobs=n_jobs)\n",
    "                    subtree_greater_equal = self.fit(new_data_greater_equal, entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=discount_importance,beta=beta, n_jobs=n_jobs)\n",
    "\n",
    "                    if subtree_less_than and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\"<\" + val, best_split_stats.get_avg_confidence()), subtree_less_than.get_root()))\n",
    "                    if subtree_greater_equal and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\">=\" + val, best_split_stats.get_avg_confidence()), subtree_greater_equal.get_root()))\n",
    "                    root.set_type(Attribute.TYPE_NUMERICAL)\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "        if len(root.get_edges()) == 0:\n",
    "            root.set_att(data.get_class_attribute().get_name())\n",
    "            root.set_type(data.get_class_attribute().get_type())\n",
    "\n",
    "        self.tree = Tree(root)\n",
    "        return self.tree\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_split_criterion( values, data, attribute, stats, globalEntropy, entropyEvaluator,min_impurity_decrease, beta=1, shap=False):\n",
    "        temp_gain = 0\n",
    "        temp_numeric_gain = 0\n",
    "        pure_temp_gain=0\n",
    "        local_info_gain = 0\n",
    "        value_to_split_on = None\n",
    "        best_split = None\n",
    "        \n",
    "        def get_maximum_label(shapdict):\n",
    "            return max(shapdict, key=shapdict.get)\n",
    "        \n",
    "        if shap:\n",
    "            labels = [get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in data.instances]\n",
    "            globalShaptropy = entropyEvaluator.calculate_raw_entropy(labels)\n",
    "        \n",
    "        for v in values:  \n",
    "            subdata = None\n",
    "            subdataLessThan = None\n",
    "            subdataGreaterEqual = None\n",
    "            if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                subdata = data.filter_nominal_attribute_value(attribute, v)\n",
    "            elif attribute.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                subdata_less_than,subdata_greater_equal = data.filter_numeric_attribute_value(attribute, v)\n",
    "            if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                stat_for_value = len(subdata)/len(data)\n",
    "                temp_gain += (stat_for_value) * entropyEvaluator.calculate_entropy(subdata)\n",
    "                if shap:\n",
    "                    shap_subdata = [get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in subdata.instances]\n",
    "                    temp_shapgain += (stat_for_value) * entropyEvaluator.calculate_raw_entropy(shap_subdata)                        \n",
    "            elif attribute.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                stat_for_lt_value = len(subdata_less_than)/len(data)\n",
    "                stat_for_gte_value = len(subdata_greater_equal)/len(data)\n",
    "                conf_for_value = stats.get_avg_confidence()\n",
    "                avg_abs_importance = stats.get_avg_abs_importance()\n",
    "                if shap:\n",
    "                    shap_less_than = [get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in subdata_less_than.instances]\n",
    "                    shap_gte_than = [get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in subdata_greater_equal.instances]\n",
    "        \n",
    "                    shaptropy_lt = entropyEvaluator.calculate_raw_entropy(shap_less_than)\n",
    "                    shaptropy_gte = entropyEvaluator.calculate_raw_entropy(shap_gte_than)\n",
    "\n",
    "                    pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+\n",
    "                                                                                           (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))\n",
    "\n",
    "                    pure_single_temp_gain_shap = (globalShaptropy   -(stat_for_lt_value*shaptropy_lt+stat_for_gte_value*shaptropy_gte))\n",
    "                    \n",
    "                    rescaled_conf = pure_single_temp_gain_shap*avg_abs_importance\n",
    "                    if pure_single_temp_gain*rescaled_conf == 0:\n",
    "                        #to prevent from 0-division\n",
    "                        single_temp_gain=0\n",
    "                    else:\n",
    "                        single_temp_gain = ((1+beta**2)*rescaled_conf*pure_single_temp_gain)/((beta**2*rescaled_conf)+pure_single_temp_gain)*conf_for_value\n",
    "                else:\n",
    "                    pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+\n",
    "                                                                                           (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))\n",
    "                    single_temp_gain = pure_single_temp_gain*conf_for_value\n",
    "                    \n",
    "                    \n",
    "                if single_temp_gain > temp_numeric_gain:\n",
    "                    temp_numeric_gain = single_temp_gain\n",
    "                    temp_gain = single_temp_gain\n",
    "                    pure_temp_gain= pure_single_temp_gain\n",
    "                    value_to_split_on = v\n",
    "                    \n",
    "        if attribute.get_type() == Attribute.TYPE_NOMINAL:\n",
    "            conf_for_value = stats.get_avg_confidence()\n",
    "            pure_temp_gain=globalEntropy-temp_gain\n",
    "            if shap:\n",
    "                avg_abs_importance = stats.get_avg_abs_importance()\n",
    "                pure_temp_gain_shap = globalShaptropy-temp_shapgain\n",
    "                rescaled_conf = pure_temp_gain_shap*avg_abs_importance\n",
    "                temp_gain = ((1+beta**2)*rescaled_conf*pure_temp_gain)/((beta**2*rescaled_conf)+pure_temp_gain)*conf_for_value\n",
    "            else:\n",
    "                temp_gain = conf_for_value*pure_temp_gain\n",
    "        if temp_gain > local_info_gain and (pure_temp_gain/globalEntropy)>=min_impurity_decrease:\n",
    "            best_split = attribute\n",
    "            \n",
    "        return best_split, value_to_split_on, temp_gain, pure_temp_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_uncertain_nominal() -> None:\n",
    "        data = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "        test = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "\n",
    "        t = UId3.fit(data, UncertainEntropyEvaluator(), 0)\n",
    "        br = TreeEvaluator.train_and_test(t, test)\n",
    "\n",
    "        print(\"###############################################################\")\n",
    "        print(f\"Correctly classified instances: {br.get_accuracy() * 100}%\")\n",
    "        print(f\"Incorrectly classified instances: {(1-br.get_accuracy()) * 100}%\")\n",
    "        print(\"TP Rate\", \"FP Rate\", \"Precision\", \"Recall\", \"F-Measure\", \"ROC Area\", \"Class\")\n",
    "\n",
    "        for class_label in data.get_class_attribute().get_domain():\n",
    "            cs = br.get_stats_for_label(class_label)\n",
    "            print(cs.get_TP_rate(), cs.get_FP_rate(), cs.get_precision(), cs.get_recall(), cs.get_F_measure(),\n",
    "                                cs.get_ROC_area(br), cs.get_class_label())\n",
    "\n",
    "    def predict(self, X):   # should take array-like X -> predict(X)\n",
    "        if not isinstance(X, (list, np.ndarray)):\n",
    "            X = [X]\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            att_stats = self.tree.predict(instance)\n",
    "            predictions.append(att_stats.get_most_probable())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cce47f",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d6929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if outlook (play= {no[0.36],yes[0.64]})\n",
      "|is  overcast[0.94] then play = {yes[1.0],no[0.0]} \n",
      "|is  sunny[0.94]then if humidity (play= {yes[0.5],no[0.5]} )\n",
      "|-------is  normal[0.93] then play = {yes[1.0],no[0.0]} \n",
      "|-------is  high[0.93] then play = {yes[0.4],no[0.6]} \n",
      "|is  rainy[0.94]then if windy (play= {no[0.4],yes[0.6]} )\n",
      "|-------is  TRUE[0.94] then play = {no[0.67],yes[0.33]} \n",
      "|-------is  FALSE[0.94] then play = {yes[1.0],no[0.0]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#instance = data.instances[0]   # powinno zwrocic pierwszy wiersz \"tabeli\"\n",
    "type_t=\"str\"\n",
    "data = Data.parse_uarff(\"../resources/weather.nominal.uncertain.arff\")\n",
    "\n",
    "uid3 = UId3()\n",
    "t = uid3.fit(data, depth=0, entropyEvaluator=UncertainEntropyEvaluator())\n",
    "if type_t == \"dot\":\n",
    "    result = t.to_dot()\n",
    "    print(result)\n",
    "    s = Source(result, filename=\"test.gv\", format=\"png\")\n",
    "    s.view()\n",
    "elif type_t == \"str\": \n",
    "    print(str(t))\n",
    "elif type_t == \"hmr\":\n",
    "    print(t.to_HMR())\n",
    "elif type_t == \"infogain\":\n",
    "    print(t.get_importances())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
