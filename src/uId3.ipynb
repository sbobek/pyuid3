{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp uid3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UId3\n",
    "\n",
    "> Class implementing UId3 algorithm for growing decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from graphviz import Source\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyuid3.attribute import Attribute\n",
    "from pyuid3.data import Data\n",
    "from pyuid3.entropy_evaluator import EntropyEvaluator\n",
    "from pyuid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator\n",
    "from pyuid3.tree import Tree\n",
    "from pyuid3.tree_node import TreeNode\n",
    "from pyuid3.tree_edge import TreeEdge\n",
    "from pyuid3.tree_evaluator import TreeEvaluator\n",
    "from pyuid3.value import Value\n",
    "from pyuid3.reading import Reading\n",
    "from pyuid3.instance import Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UId3(BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth=2, node_size_limit = 1, grow_confidence_threshold = 0):\n",
    "        self.TREE_DEPTH_LIMIT= max_depth\n",
    "        self.NODE_SIZE_LIMIT = node_size_limit\n",
    "        self.GROW_CONFIDENCE_THRESHOLD = grow_confidence_threshold\n",
    "        self.tree = None\n",
    "        self.node_size_limit = node_size_limit\n",
    "\n",
    "    def fit(self, data, y=None, *, depth,  entropyEvaluator, discount_importance = False):   # data should be split into array-like X and y and then fit should be 'fit(X, y)':\n",
    "        if len(data.get_instances()) < self.NODE_SIZE_LIMIT:\n",
    "            return None\n",
    "        if depth > self.TREE_DEPTH_LIMIT:\n",
    "            return None\n",
    "        entropy = UncertainEntropyEvaluator().calculate_entropy(data)\n",
    "\n",
    "        data.update_attribute_domains()\n",
    "\n",
    "        # of the set is heterogeneous or no attributes to split, just class -- return\n",
    "        # leaf\n",
    "        if entropy == 0 or len(data.get_attributes()) == 1:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            result = Tree(root)\n",
    "            return result\n",
    "\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        \n",
    "        \n",
    "        cl=[]\n",
    "        for i in data.get_instances():\n",
    "            cl.append(i.get_reading_for_attribute(data.get_class_attribute()).get_most_probable().get_name())\n",
    "        \n",
    "        for a in data.get_attributes():\n",
    "            if data.get_class_attribute() == a:\n",
    "                continue\n",
    "            values = a.get_domain()\n",
    "            temp_gain = entropy\n",
    "            temp_numeric_gain = 0\n",
    "            stats = data.calculate_statistics(a)\n",
    "            \n",
    "            ## start searching for best border values  -- such that class value remains the same for the ranges between them\n",
    "            if a.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                border_search_list = []\n",
    "                for i in data.get_instances():\n",
    "                    v=i.get_reading_for_attribute(a).get_most_probable().get_name()\n",
    "                    border_search_list.append([v])\n",
    "                border_search_df = pd.DataFrame(border_search_list,columns=['values'])\n",
    "                border_search_df['values']=border_search_df['values'].astype('f8')\n",
    "                border_search_df['class'] = cl\n",
    "                border_search_df=border_search_df.sort_values(by='values')\n",
    "                border_search_df['class_shitf'] = border_search_df['class'].shift(1)\n",
    "                values_a = border_search_df[border_search_df['class_shitf'] != border_search_df['class']]['values'].astype('str')\n",
    "                border_search_df['class_shitf'] = border_search_df['class'].shift(-1)\n",
    "                values_b=border_search_df[border_search_df['class_shitf'] != border_search_df['class']]['values'].astype('str')\n",
    "                values = np.unique(np.concatenate((values_a,values_b)))\n",
    "            ## stop searching for best border values\n",
    "            for v in values:  \n",
    "                subdata = None\n",
    "                subdataLessThan = None\n",
    "                subdataGreaterEqual = None\n",
    "                if a.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                    subdata = data.filter_nominal_attribute_value(a, v)\n",
    "                elif a.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                    subdata_less_than,subdata_greater_equal = data.filter_numeric_attribute_value(a, v)\n",
    "                if a.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                    temp_gain -= (stats.get_stat_for_value(v)) * entropyEvaluator.calculate_entropy(subdata) \n",
    "                elif a.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                    stat_for_lt_value = len(subdata_less_than)/len(data)\n",
    "                    stat_for_gte_value = len(subdata_greater_equal)/len(data)\n",
    "                    single_temp_gain = stats.get_avg_confidence()*(entropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than) + (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal)))\n",
    "                    if single_temp_gain >= temp_numeric_gain:\n",
    "                        temp_numeric_gain = single_temp_gain\n",
    "                        temp_gain = single_temp_gain\n",
    "                        a.set_value_to_split_on(v) \n",
    "            if temp_gain >= info_gain:\n",
    "                info_gain = temp_gain\n",
    "                best_split = a\n",
    "                a.set_importance_gain(info_gain) \n",
    "        # if nothing better can happen\n",
    "        if best_split == None:\n",
    "            # create the only node and summary for it\n",
    "            class_att = data.get_class_attribute()\n",
    "            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))\n",
    "            root.set_type(class_att.get_type())\n",
    "            result = Tree(root)\n",
    "            return result\n",
    "        # Create root node, and recursively go deeper into the tree.\n",
    "        class_att = data.get_class_attribute()\n",
    "        class_stats = data.calculate_statistics(class_att)\n",
    "        root = TreeNode(best_split.get_name(), class_stats)\n",
    "        root.set_type(class_att.get_type())\n",
    "\n",
    "        # attach newly created trees\n",
    "        for val in best_split.get_splittable_domain():\n",
    "            if best_split.get_type() == Attribute.TYPE_NOMINAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data = data.filter_nominal_attribute_value(best_split, val)\n",
    "                if discount_importance:\n",
    "                    new_data = new_data.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "\n",
    "                subtree = self.fit(new_data, entropyEvaluator=entropyEvaluator, depth=depth + 1)\n",
    "                \n",
    "                if subtree and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                    root.add_edge(TreeEdge(Value(val, best_split_stats.get_avg_confidence()), subtree.get_root()))\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "            elif best_split.get_type() == Attribute.TYPE_NUMERICAL:\n",
    "                best_split_stats = data.calculate_statistics(best_split)\n",
    "                new_data_less_then,new_data_greater_equal = data.filter_numeric_attribute_value(best_split, val)\n",
    "                \n",
    "                if discount_importance:\n",
    "                    new_data_less_then = new_data_less_then.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                    new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)\n",
    "                \n",
    "                if len(new_data_less_then) >= self.node_size_limit and len(new_data_greater_equal) >= self.node_size_limit:\n",
    "                    subtree_less_than = self.fit(new_data_less_then, entropyEvaluator=entropyEvaluator, depth=depth + 1)\n",
    "                    subtree_greater_equal = self.fit(new_data_greater_equal, entropyEvaluator=entropyEvaluator, depth=depth + 1)\n",
    "                    if subtree_less_than and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\"<\" + val, best_split_stats.get_avg_confidence()), subtree_less_than.get_root()))\n",
    "                    if subtree_greater_equal and best_split_stats.get_most_probable().get_confidence() > self.GROW_CONFIDENCE_THRESHOLD:\n",
    "                        root.add_edge(TreeEdge(Value(\">=\" + val, best_split_stats.get_avg_confidence()), subtree_greater_equal.get_root()))\n",
    "                    root.set_type(Attribute.TYPE_NUMERICAL)\n",
    "                    root.set_infogain(best_split.get_importance_gain())\n",
    "\n",
    "        if len(root.get_edges()) == 0:\n",
    "            root.set_att(data.get_class_attribute().get_name())\n",
    "            root.set_type(data.get_class_attribute().get_type())\n",
    "\n",
    "        self.tree = Tree(root)\n",
    "        return self.tree\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_uncertain_nominal() -> None:\n",
    "        data = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "        test = Data.parse_uarff(\"../resources/machine.nominal.uncertain.arff\")\n",
    "\n",
    "        t = UId3.fit(data, UncertainEntropyEvaluator(), 0)\n",
    "        br = TreeEvaluator.train_and_test(t, test)\n",
    "\n",
    "        print(\"###############################################################\")\n",
    "        print(f\"Correctly classified instances: {br.get_accuracy() * 100}%\")\n",
    "        print(f\"Incorrectly classified instances: {(1-br.get_accuracy()) * 100}%\")\n",
    "        print(\"TP Rate\", \"FP Rate\", \"Precision\", \"Recall\", \"F-Measure\", \"ROC Area\", \"Class\")\n",
    "\n",
    "        for class_label in data.get_class_attribute().get_domain():\n",
    "            cs = br.get_stats_for_label(class_label)\n",
    "            print(cs.get_TP_rate(), cs.get_FP_rate(), cs.get_precision(), cs.get_recall(), cs.get_F_measure(),\n",
    "                                cs.get_ROC_area(br), cs.get_class_label())\n",
    "\n",
    "    def predict(self, X):   # should take array-like X -> predict(X)\n",
    "        if not isinstance(X, (list, np.ndarray)):\n",
    "            X = [X]\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            att_stats = self.tree.predict(instance)\n",
    "            predictions.append(att_stats.get_most_probable())\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if outlook (play= {no[0.36],yes[0.64]})\n",
      "|is  overcast[0.94] then play = {yes[1.0],no[0.0]} \n",
      "|is  sunny[0.94]then if humidity (play= {yes[0.5],no[0.5]} )\n",
      "|-------is  normal[0.93] then play = {yes[1.0],no[0.0]} \n",
      "|-------is  high[0.93] then play = {yes[0.4],no[0.6]} \n",
      "|is  rainy[0.94]then if windy (play= {no[0.4],yes[0.6]} )\n",
      "|-------is  TRUE[0.94] then play = {no[0.67],yes[0.33]} \n",
      "|-------is  FALSE[0.94] then play = {yes[1.0],no[0.0]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#instance = data.instances[0]   # powinno zwrocic pierwszy wiersz \"tabeli\"\n",
    "type_t=\"str\"\n",
    "data = Data.parse_uarff(\"../resources/weather.nominal.uncertain.arff\")\n",
    "\n",
    "uid3 = UId3()\n",
    "t = uid3.fit(data, depth=0, entropyEvaluator=UncertainEntropyEvaluator())\n",
    "if type_t == \"dot\":\n",
    "    result = t.to_dot()\n",
    "    print(result)\n",
    "    s = Source(result, filename=\"test.gv\", format=\"png\")\n",
    "    s.view()\n",
    "elif type_t == \"str\": \n",
    "    print(str(t))\n",
    "elif type_t == \"hmr\":\n",
    "    print(t.to_HMR())\n",
    "elif type_t == \"infogain\":\n",
    "    print(t.get_importances())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
